{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialize project environemnt and create an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name='Tennis.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment, two agents control rackets to bounce a ball over a net. Thust he observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from hyperparameters  import *\n",
    "from agent import Agent\n",
    "\n",
    "def seeding(seed=1):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seeding(RANDOM_SEED)\n",
    "agent_0 = Agent(state_size, action_size, random_seed=RANDOM_SEED)\n",
    "agent_1 = Agent(state_size, action_size, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train the agent and store trained model weights into checkpoint.pth file.\n",
    "\n",
    "FYI: Instead of training the agent from sracth to run in project environment, it is possible to load the stored model weights of already trained agent from please skip steps 2 and 3 and jump directly to step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def maddpg(n_episodes=2000, max_t=1000, print_every=10):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    avg_scores = []\n",
    "    for episode_id in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        agent_0.reset()\n",
    "        agent_1.reset()\n",
    "        agent_scores = np.zeros(num_agents) # storage for this episode scores for both agaents\n",
    "\n",
    "        for t in range(max_t):\n",
    "            action_0 = agent_1.act(states[0], add_noise=True)\n",
    "            action_1 = agent_1.act(states[1], add_noise=True)\n",
    "            env_info = env.step([action_0, action_1])[brain_name] # send  actions of both agents to the environment\n",
    "            next_states = env_info.vector_observations            # get next state (for each agent)\n",
    "            rewards = env_info.rewards                            # get reward (for each agent)\n",
    "            dones = env_info.local_done                           # see if episode finished\n",
    "\n",
    "            agent_0.step(states[0], action_0, rewards[0], next_states[0], dones[0])\n",
    "            agent_1.step(states[1], action_1, rewards[1], next_states[1], dones[1])\n",
    "\n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            if np.any(dones):\n",
    "                break\n",
    "\n",
    "        episode_score = np.max(agent_scores)  # Consider the maximum score amongs all Agents\n",
    "        scores_deque.append(episode_score)\n",
    "        scores.append(episode_score)\n",
    "        eval_window_avg_score = np.mean(scores_deque)\n",
    "        avg_scores.append(eval_window_avg_score)\n",
    "        total_episodes = min(episode_id , 100)\n",
    "        print('\\rEpisode {} average score: {:.2f}, Average score over last {} episodes: {:.2f}'.format(i_episode, np.mean(score), total_episodes, eval_window_avg_score), end=\"\")\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {} average score: {:.2f}, Average score over last {} episodes: {:.2f}'.format(i_episode, np.mean(score), total_episodes, eval_window_avg_score))\n",
    "        if np.mean(scores_deque) >= 30:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\t Average score over last {} episodes: {:.2f}'.format(i_episode, total_episodes, eval_window_avg_score))\n",
    "            torch.save(agent_0.actor_local.state_dict(), 'checkpoint_actor_0.pth')\n",
    "            torch.save(agent_0.critic_local.state_dict(), 'checkpoint_critic_0.pth')\n",
    "            torch.save(agent_1.actor_local.state_dict(), 'checkpoint_actor_1.pth')\n",
    "            torch.save(agent_1.critic_local.state_dict(), 'checkpoint_critic_1.pth')\n",
    "            break\n",
    "    return scores, avg_scores\n",
    "\n",
    "scores, avg_scores = execute_maddpg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run the Agents with pretrained model weights\n",
    "\n",
    "Once this cell is executed, a Unity window with loaded project environemnt should pop up where the trained Agents can be observed in action, as they operate inside the environment. The 'experience' of the Agents is loaded from stored ''checkpoint_actor.pth' file. These files contains the weights of Agents' neural netowrk model learned during training phase, therefore no need to train the Agents every time from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                    # get the current states\n",
    "agent.reset()\n",
    "agent.actor_local.load_state_dict(torch.load('actor_checkpoint.pth'))\n",
    "while True:\n",
    "    actions = agent.act(states, False)            # let the agent select actions (don't add noise)\n",
    "    env_info = env.step(actions)[brain_name]      # execute the selected actions and save the new information about the environment\n",
    "    next_states = env_info.vector_observations    # get the resulting states\n",
    "    dones = env_info.local_done                   # check whether episodes have finished\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop the game, close the Unity window with project environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7042e7523171f2a1e37a031fc61f6d1b03fe8cfad426cc0438c624b42620eab4"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('drlnd': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
